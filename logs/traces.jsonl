{"question": "What is parametric memory?", "plan": {"objective": "What is parametric memory?", "steps": [{"step_id": 1, "action": "noop", "args": {}, "rationale": "conceptual/mechanistic answer likely parametric"}]}, "execution": [{"step_id": 1, "action": "noop", "args": {}, "tool_result": {"status": "skipped", "context": ""}, "result_meta": {"num_chunks": 0}}], "final_answer": "(Answer from parametric knowledge)\n\nQ: What is parametric memory?\n\n[no retrieval used]", "memory": {"semantic_reads": {"last_user_question": null}, "episodic_reads": {"recent_episodes_n": 0}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "What is parametric memory?", "thoughts": ["Planner evaluating retrieval need for: What is parametric memory?", "Executed step 1: noop"], "flags": {}}, "policy_mode": {"retrieval_advice": null, "policy_enforced": true}}
{"question": "What is parametric memory according to Attention Is All You Need?", "plan": {"objective": "What is parametric memory according to Attention Is All You Need?", "steps": [{"step_id": 1, "action": "retrieve", "args": {"question": "What is parametric memory according to Attention Is All You Need?", "k": 4}, "rationale": "evidence-dependent request (source-specific)"}]}, "execution": [{"step_id": 1, "action": "retrieve", "args": {"question": "What is parametric memory according to Attention Is All You Need?", "k": 4}, "tool_result": {"k": 4, "mode": "hybrid", "reranked": false, "candidate_pool_size": 20, "chunks": [{"chunk_id": 0, "text": "Attention Is All You Need AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗ GoogleBrain GoogleBrain GoogleResearch GoogleResearch avaswani@google.com noam@google.com nikip@google.com usz@google.com LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗ GoogleResearch UniversityofToronto GoogleBrain llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com IlliaPolosukhin∗ ‡ illia.polosukhin@gmail.com Abstract Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetwo", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 1, "sparse_score": 17.478273784912375}, "source": "Attention Is All You Need.pdf"}, {"chunk_id": 631, "text": "ranAssociates,Inc.,2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. [59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David Crandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes. AAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17329. [60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman. GLUE: A multi-task benchmark and analysis platform", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 2, "sparse_score": 16.831041013504354}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 413, "text": "ification:acomprehensive [22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: review,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40, A neural image caption generator,” in Proceedings of the IEEE 2021. conference on computer vision and pattern recognition, 2015, pp. [44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. 3156–3164. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” [23] M.E.Peters,M.Neumann,M.Iyyer,M.Gardner,C.Clark,K.Lee, Ad", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 3, "sparse_score": 13.525398006180717}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 586, "text": " includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingfuturework. 6 Discussion Inthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric memory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We foundthatpeoplepreferRAG’sge", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 4, "sparse_score": 11.332052704919354}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}]}, "result_meta": {"num_chunks": 4}}], "final_answer": "(Answer conditioned on retrieved context)\n\nQ: What is parametric memory according to Attention Is All You Need?\n\n[context used]\n[0] Attention Is All You Need AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗ GoogleBrain GoogleBrain GoogleResearch GoogleResearch avaswani@google.com noam@google.com nikip@google.com usz@google.com LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗ GoogleResearch UniversityofToronto GoogleBrain llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com IlliaPolosukhin∗ ‡ illia.polosukhin@gmail.com Abstract Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetwo\n\n[631] ranAssociates,Inc.,2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. [59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David Crandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes. AAAIConferenceonArtificialIntel", "memory": {"semantic_reads": {"last_user_question": "What is parametric memory?"}, "episodic_reads": {"recent_episodes_n": 1}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "What is parametric memory according to Attention Is All You Need?", "thoughts": ["Planner evaluating retrieval need for: What is parametric memory according to Attention Is All You Need?", "Executed step 1: retrieve"], "flags": {"used_retrieval": true}}, "policy_mode": {"retrieval_advice": null, "policy_enforced": true}}
{"question": "What is parametric memory?", "plan": {"objective": "What is parametric memory?", "steps": [{"step_id": 1, "action": "retrieve", "args": {"question": "What is parametric memory?", "k": 4}, "rationale": "retrieval chosen despite parametric uncertainty due to memory signal"}]}, "execution": [{"step_id": 1, "action": "retrieve", "args": {"question": "What is parametric memory?", "k": 4}, "tool_result": {"k": 4, "mode": "hybrid", "reranked": false, "candidate_pool_size": 20, "chunks": [{"chunk_id": 586, "text": " includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingfuturework. 6 Discussion Inthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric memory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We foundthatpeoplepreferRAG’sge", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 1, "sparse_score": 11.332052704919354}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 528, "text": "eq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned. Therehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric memorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive knowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgei", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 2, "sparse_score": 9.952775400808168}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 253, "text": "nstotheirabilities,there or even beneficial. are still some important limitations that come up, particularly if they are used naively. Some of them include the following: LLMs, trained on diverse datasets including the internet, books, and Wikipedia, generate text based on probabilistic • They don’t have state/memory. LLMs on their own models without an inherent understanding of truth or falsity. cannot remember even what was sent to them in the Recent advancements like instruct tuning and Reinf", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 3, "sparse_score": 6.7957638224041785}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 237, "text": "Optimizer (ZeRO), to optimize memory, vastly improving training speed of LLMs while 2)Low-Rank Adaption (LoRA): Low-Rank Adaptation is increasingthemodelsizethatcanbeefficientlytrained.ZeRO a popular and lightweight training technique that significantly eliminates memory redundancies in data- and model-parallel reduces the number of trainable parameters, and is based training while retaining low communication volume and high on a crucial insight that the difference between the fine- computationa", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 4, "sparse_score": 6.370606927421563}, "source": "Large Language Models A Survey.pdf"}]}, "result_meta": {"num_chunks": 4}}], "final_answer": "(Answer conditioned on retrieved context)\n\nQ: What is parametric memory?\n\n[context used]\n[586]  includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingfuturework. 6 Discussion Inthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric memory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We foundthatpeoplepreferRAG’sge\n\n[528] eq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned. Therehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric memorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack- augmented networks [25] and memory layers [30].", "memory": {"semantic_reads": {"last_user_question": "What is parametric memory according to Attention Is All You Need?"}, "episodic_reads": {"recent_episodes_n": 2}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "What is parametric memory?", "thoughts": ["Planner evaluating retrieval need for: What is parametric memory?", "Memory advises retrieval", "Executed step 1: retrieve"], "flags": {"used_retrieval": true}}, "policy_mode": {"retrieval_advice": true, "policy_enforced": true}}
{"question": "Summarize the difference between parametric and non-parametric memory.", "plan": {"objective": "Summarize the difference between parametric and non-parametric memory.", "steps": [{"step_id": 1, "action": "retrieve", "args": {"question": "Summarize the difference between parametric and non-parametric memory.", "k": 4}, "rationale": "evidence-dependent request (source-specific)"}]}, "execution": [{"step_id": 1, "action": "retrieve", "args": {"question": "Summarize the difference between parametric and non-parametric memory.", "k": 4}, "tool_result": {"k": 4, "mode": "hybrid", "reranked": false, "candidate_pool_size": 20, "chunks": [{"chunk_id": 84, "text": " open source NLM Previousresearchhasfocusedondevelopingagentsforspecific toolkit) by Mikolov [41], [42] helped significantly popularize tasks and domains. The emergent abilities demonstrated by NLMs.Afterwards,NLMsbasedonrecurrentneuralnetworks LLMs make it possible to build general-purpose AI agents (RNNs) and their variants, such as long short-term memory basedonLLMs.WhileLLMsaretrainedtoproduceresponses (LSTM)[19]andgatedrecurrentunit(GRU)[20],werewidely instaticsettings,AIagentsneedtotakeact", "score": {"priority": 1, "dense_rank": 17, "dense_score": 0.7297849534463752, "sparse_rank": 16, "sparse_score": 5.042759702314706}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 586, "text": " includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingfuturework. 6 Discussion Inthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric memory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We foundthatpeoplepreferRAG’sge", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 1, "sparse_score": 18.168724577295116}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 526, "text": "to-sequence(seq2seq)models. Weendowpre-trained,parametric-memorygenerationmodelswithanon-parametricmemorythrough ageneral-purposefine-tuningapproachwhichwerefertoasretrieval-augmentedgeneration(RAG). WebuildRAGmodelswheretheparametricmemoryisapre-trainedseq2seqtransformer,andthe non-parametricmemoryisadensevectorindexofWikipedia,accessedwithapre-trainedneural retriever. Wecombinethesecomponentsinaprobabilisticmodeltrainedend-to-end(Fig. 1). The retriever(DensePassageRetriever[26],henceforthDPR)p", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 2, "sparse_score": 17.867352628227366}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 528, "text": "eq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned. Therehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric memorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive knowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgei", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 3, "sparse_score": 15.957323862292121}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}]}, "result_meta": {"num_chunks": 4}}], "final_answer": "(Answer conditioned on retrieved context)\n\nQ: Summarize the difference between parametric and non-parametric memory.\n\n[context used]\n[84]  open source NLM Previousresearchhasfocusedondevelopingagentsforspecific toolkit) by Mikolov [41], [42] helped significantly popularize tasks and domains. The emergent abilities demonstrated by NLMs.Afterwards,NLMsbasedonrecurrentneuralnetworks LLMs make it possible to build general-purpose AI agents (RNNs) and their variants, such as long short-term memory basedonLLMs.WhileLLMsaretrainedtoproduceresponses (LSTM)[19]andgatedrecurrentunit(GRU)[20],werewidely instaticsettings,AIagentsneedtotakeact\n\n[586]  includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingf", "memory": {"semantic_reads": {"last_user_question": "What is parametric memory?"}, "episodic_reads": {"recent_episodes_n": 3}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "Summarize the difference between parametric and non-parametric memory.", "thoughts": ["Planner evaluating retrieval need for: Summarize the difference between parametric and non-parametric memory.", "Executed step 1: retrieve"], "flags": {"used_retrieval": true}}, "policy_mode": {"retrieval_advice": false, "policy_enforced": false}}
{"question": "Explain that difference again, briefly.", "plan": {"objective": "Explain that difference again, briefly.", "steps": [{"step_id": 1, "action": "noop", "args": {}, "rationale": "conceptual/mechanistic answer likely parametric"}]}, "execution": [{"step_id": 1, "action": "noop", "args": {}, "tool_result": {"status": "skipped", "context": ""}, "result_meta": {"num_chunks": 0}}], "final_answer": "(Answer from parametric knowledge)\n\nQ: Explain that difference again, briefly.\n\n[no retrieval used]", "memory": {"semantic_reads": {"last_user_question": "Summarize the difference between parametric and non-parametric memory."}, "episodic_reads": {"recent_episodes_n": 4}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "Explain that difference again, briefly.", "thoughts": ["Planner evaluating retrieval need for: Explain that difference again, briefly.", "Executed step 1: noop"], "flags": {}}, "policy_mode": {"retrieval_advice": false, "policy_enforced": false}}
{"question": "What does RAG mean in large language models?", "plan": {"objective": "What does RAG mean in large language models?", "steps": [{"step_id": 1, "action": "noop", "args": {}, "rationale": "no strong evidence dependency detected"}]}, "execution": [{"step_id": 1, "action": "noop", "args": {}, "tool_result": {"status": "skipped", "context": ""}, "result_meta": {"num_chunks": 0}}], "final_answer": "(Answer from parametric knowledge)\n\nQ: What does RAG mean in large language models?\n\n[no retrieval used]", "memory": {"semantic_reads": {"last_user_question": "Explain that difference again, briefly."}, "episodic_reads": {"recent_episodes_n": 5}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "What does RAG mean in large language models?", "thoughts": ["Planner evaluating retrieval need for: What does RAG mean in large language models?", "Executed step 1: noop"], "flags": {}}, "policy_mode": {"retrieval_advice": false, "policy_enforced": false}}
{"question": "What is RAG?", "plan": {"objective": "What is RAG?", "steps": [{"step_id": 1, "action": "noop", "args": {}, "rationale": "conceptual/mechanistic answer likely parametric"}]}, "execution": [{"step_id": 1, "action": "noop", "args": {}, "tool_result": {"status": "skipped", "context": ""}, "result_meta": {"num_chunks": 0}}], "final_answer": "(Answer from parametric knowledge)\n\nQ: What is RAG?\n\n[no retrieval used]", "memory": {"semantic_reads": {"last_user_question": "What does RAG mean in large language models?"}, "episodic_reads": {"recent_episodes_n": 6}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "What is RAG?", "thoughts": ["Planner evaluating retrieval need for: What is RAG?", "Executed step 1: noop"], "flags": {}}, "policy_mode": {"retrieval_advice": false, "policy_enforced": false}}
{"question": "Summarize the difference between parametric and non-parametric memory.", "plan": {"objective": "Summarize the difference between parametric and non-parametric memory.", "steps": [{"step_id": 1, "action": "retrieve", "args": {"question": "Summarize the difference between parametric and non-parametric memory.", "k": 4}, "rationale": "evidence-dependent request (source-specific)"}]}, "execution": [{"step_id": 1, "action": "retrieve", "args": {"question": "Summarize the difference between parametric and non-parametric memory.", "k": 4}, "tool_result": {"k": 4, "mode": "hybrid", "reranked": false, "candidate_pool_size": 20, "chunks": [{"chunk_id": 84, "text": " open source NLM Previousresearchhasfocusedondevelopingagentsforspecific toolkit) by Mikolov [41], [42] helped significantly popularize tasks and domains. The emergent abilities demonstrated by NLMs.Afterwards,NLMsbasedonrecurrentneuralnetworks LLMs make it possible to build general-purpose AI agents (RNNs) and their variants, such as long short-term memory basedonLLMs.WhileLLMsaretrainedtoproduceresponses (LSTM)[19]andgatedrecurrentunit(GRU)[20],werewidely instaticsettings,AIagentsneedtotakeact", "score": {"priority": 1, "dense_rank": 17, "dense_score": 0.7297849534463752, "sparse_rank": 16, "sparse_score": 5.042759702314706}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 586, "text": " includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingfuturework. 6 Discussion Inthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric memory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We foundthatpeoplepreferRAG’sge", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 1, "sparse_score": 18.168724577295116}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 526, "text": "to-sequence(seq2seq)models. Weendowpre-trained,parametric-memorygenerationmodelswithanon-parametricmemorythrough ageneral-purposefine-tuningapproachwhichwerefertoasretrieval-augmentedgeneration(RAG). WebuildRAGmodelswheretheparametricmemoryisapre-trainedseq2seqtransformer,andthe non-parametricmemoryisadensevectorindexofWikipedia,accessedwithapre-trainedneural retriever. Wecombinethesecomponentsinaprobabilisticmodeltrainedend-to-end(Fig. 1). The retriever(DensePassageRetriever[26],henceforthDPR)p", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 2, "sparse_score": 17.867352628227366}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}, {"chunk_id": 528, "text": "eq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned. Therehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric memorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive knowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgei", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 3, "sparse_score": 15.957323862292121}, "source": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf"}]}, "result_meta": {"num_chunks": 4}}], "final_answer": "(Answer conditioned on retrieved context)\n\nQ: Summarize the difference between parametric and non-parametric memory.\n\n[context used]\n[84]  open source NLM Previousresearchhasfocusedondevelopingagentsforspecific toolkit) by Mikolov [41], [42] helped significantly popularize tasks and domains. The emergent abilities demonstrated by NLMs.Afterwards,NLMsbasedonrecurrentneuralnetworks LLMs make it possible to build general-purpose AI agents (RNNs) and their variants, such as long short-term memory basedonLLMs.WhileLLMsaretrainedtoproduceresponses (LSTM)[19]andgatedrecurrentunit(GRU)[20],werewidely instaticsettings,AIagentsneedtotakeact\n\n[586]  includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingf", "memory": {"semantic_reads": {"last_user_question": "What is RAG?"}, "episodic_reads": {"recent_episodes_n": 7}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "Summarize the difference between parametric and non-parametric memory.", "thoughts": ["Planner evaluating retrieval need for: Summarize the difference between parametric and non-parametric memory.", "Executed step 1: retrieve"], "flags": {"used_retrieval": true}}, "policy_mode": {"retrieval_advice": null, "policy_enforced": true}}
{"question": "Explain that difference again, briefly.", "plan": {"objective": "Explain that difference again, briefly.", "steps": [{"step_id": 1, "action": "retrieve", "args": {"question": "Explain that difference again, briefly.", "k": 4}, "rationale": "retrieval chosen despite parametric uncertainty due to memory signal"}]}, "execution": [{"step_id": 1, "action": "retrieve", "args": {"question": "Explain that difference again, briefly.", "k": 4}, "tool_result": {"k": 4, "mode": "hybrid", "reranked": false, "candidate_pool_size": 20, "chunks": [{"chunk_id": 228, "text": "erealworldthanpreferenceoptimizationmethods,as 3)Top-k Sampling: Top-k sampling is a technique that thekindofdataitneedsisfarmoreabundant.Asanexample, uses the probability distribution generated by the language everyretailcompanyhasalotofcustomerinteractiondataand model to select a token randomly from the k most likely whether that interaction was successful (e.g., purchase made) options. orunsuccessful(e.g.,nopurchasemade).However,Theyhave little to no counterfactual data (i.e., what would have", "score": {"priority": 1, "dense_rank": 11, "dense_score": 0.5644001707709828, "sparse_rank": 19, "sparse_score": 2.0782635939861107}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 237, "text": "Optimizer (ZeRO), to optimize memory, vastly improving training speed of LLMs while 2)Low-Rank Adaption (LoRA): Low-Rank Adaptation is increasingthemodelsizethatcanbeefficientlytrained.ZeRO a popular and lightweight training technique that significantly eliminates memory redundancies in data- and model-parallel reduces the number of trainable parameters, and is based training while retaining low communication volume and high on a crucial insight that the difference between the fine- computationa", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 1, "sparse_score": 7.50664819477738}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 238, "text": "that the difference between the fine- computational granularity, allowing one to scale the model tuned weights for a specialized task and the initial pre-trained sizeproportionaltothenumberofdeviceswithsustainedhigh weights often exhibits “low intrinsic rank” - meaning that efficiency. it can be approximated well by a low rank matrix [142].\nFig. 35: A generic knowledge distillation framework with student and teacher (Courtesy of [144]). Fig. 34: An illustration of LoRA reparametrizan. Only A and", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 2, "sparse_score": 7.367814563983668}, "source": "Large Language Models A Survey.pdf"}, {"chunk_id": 380, "text": "ly,there are still numerous challenges ahead. Here we briefly mention has been promising research in alternative approaches that are someofthechallengesandmainactiveareaswhichareknown being labelled as post-attention. so far. It is worth noting that LLM challenges are discussed An important class of such class of post-attention models in details in a work by Kaddour et al. [207]. arethesocalledStateSpaceModels(SSMs).Whilethenotion of State Space Models has a long history in machine learning, A. ", "score": {"priority": 2, "dense_rank": null, "dense_score": null, "sparse_rank": 3, "sparse_score": 7.278126498163387}, "source": "Large Language Models A Survey.pdf"}]}, "result_meta": {"num_chunks": 4}}], "final_answer": "(Answer conditioned on retrieved context)\n\nQ: Explain that difference again, briefly.\n\n[context used]\n[228] erealworldthanpreferenceoptimizationmethods,as 3)Top-k Sampling: Top-k sampling is a technique that thekindofdataitneedsisfarmoreabundant.Asanexample, uses the probability distribution generated by the language everyretailcompanyhasalotofcustomerinteractiondataand model to select a token randomly from the k most likely whether that interaction was successful (e.g., purchase made) options. orunsuccessful(e.g.,nopurchasemade).However,Theyhave little to no counterfactual data (i.e., what would have\n\n[237] Optimizer (ZeRO), to optimize memory, vastly improving training speed of LLMs while 2)Low-Rank Adaption (LoRA): Low-Rank Adaptation is increasingthemodelsizethatcanbeefficientlytrained.ZeRO a popular and lightweight training technique that significantly eliminates memory redundancies i", "memory": {"semantic_reads": {"last_user_question": "Summarize the difference between parametric and non-parametric memory."}, "episodic_reads": {"recent_episodes_n": 8}, "semantic_writes": ["last_user_question", "last_answer_preview"], "episodic_writes": "append"}, "working_memory": {"goal": "Explain that difference again, briefly.", "thoughts": ["Planner evaluating retrieval need for: Explain that difference again, briefly.", "Memory advises retrieval", "Executed step 1: retrieve"], "flags": {"used_retrieval": true}}, "policy_mode": {"retrieval_advice": true, "policy_enforced": true}}
